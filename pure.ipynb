{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load required libraries and frameworks"
      ],
      "metadata": {
        "id": "iJsIToersvdr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRbY7w7aPaGW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "import scipy.ndimage as nd\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from keras.preprocessing import image\n",
        "import pickle\n",
        "import random, os\n",
        "from google.colab import drive, files\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from scipy.special import softmax\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "X7n9mmcdd0Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to run the following first and restart runtime\n",
        "!pip install h5py==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo8XiGhsYWxZ",
        "outputId": "22b604e7-bb4e-433c-8b2c-b04c73f54af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from h5py==2.10.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from h5py==2.10.0) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed value\n",
        "seed = 333\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n"
      ],
      "metadata": {
        "id": "seAJsIx-PdhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories that you can find the dataset and save your trained model\n",
        "dataset_dir = '/dataset_dir'\n",
        "model_dir = '/model_dir'\n"
      ],
      "metadata": {
        "id": "shdHg_9TF6Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train_Model = False if you use a trained model\n",
        "Train_Model = True\n",
        "batch_size = 100\n",
        "INIT_LR = 1e-5\n",
        "NUM_EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "eK_2Q5L1UoSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(dataset_dir + \"/df_train.csv\")\n",
        "df_train = df_train.drop_duplicates()\n",
        "df_test = pd.read_csv(dataset_dir +\"/df_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taal9dzTIbmR",
        "outputId": "e66d8de1-d6a0-41fb-aea8-d0f51e043acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19167, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def append_ext(fn):\n",
        "    return str(fn)+\".jpg\"\n",
        "\n",
        "def prepare_df(df_train_dataset, df_test_dataset):\n",
        "    df_train_dataset[\"photoid\"] = df_train_dataset[\"photoid\"].apply(append_ext)\n",
        "    df_test_dataset[\"photoid\"] = df_test_dataset[\"photoid\"].apply(append_ext)\n",
        "    df_train_dataset['privacy_value'] = df_train_dataset.apply(lambda row: \"public\" if row.normalizedpublic==1 else \"private\", axis=1)\n",
        "    df_test_dataset['privacy_value'] = df_test_dataset.apply(lambda row: \"public\" if row.normalizedpublic==1 else \"private\", axis=1)\n",
        "\n",
        "    return df_train_dataset, df_test_dataset\n"
      ],
      "metadata": {
        "id": "IdJ1sFtoYt3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_dataset, df_test_dataset = prepare_df(df_train, df_test)\n"
      ],
      "metadata": {
        "id": "Gblai3hVYvZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Datasets"
      ],
      "metadata": {
        "id": "fuAve0Ewsz4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test generators from a dataframe\n",
        "trainAug = ImageDataGenerator(\n",
        "\trotation_range=25,\n",
        "\tzoom_range=0.1,\n",
        "\twidth_shift_range=0.1,\n",
        "\theight_shift_range=0.1,\n",
        "\tshear_range=0.2,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "train_generator=trainAug.flow_from_dataframe(\n",
        "    dataframe = df_train_dataset,\n",
        "    directory = dataset_dir + \"/ImFiles\",\n",
        "    x_col = \"photoid\",\n",
        "    y_col = \"privacy_value\",\n",
        "    target_size = (224, 224),\n",
        "    color_mode = 'rgb',\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = batch_size,\n",
        "    seed = seed,\n",
        "    shuffle = True,\n",
        "    save_to_dir = None,\n",
        "    save_prefix = '',\n",
        "    save_format = 'png',\n",
        "    follow_links = False,\n",
        "    subset = None,\n",
        "    interpolation = 'nearest')\n",
        "\n",
        "test_generator = ImageDataGenerator().flow_from_dataframe(\n",
        "    dataframe = df_test_dataset,\n",
        "    directory = dataset_dir + \"/ImFiles\",\n",
        "    x_col = \"photoid\",\n",
        "    y_col = None,\n",
        "    class_mode = None,\n",
        "    batch_size = batch_size,\n",
        "    seed = seed,\n",
        "    shuffle = False,\n",
        "    target_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "r19z-jmFPtMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998b2f56-40c7-4c9c-e66f-c72e462d1ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19167 validated image filenames belonging to 2 classes.\n",
            "Found 5000 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totalTrain = train_generator.n//train_generator.batch_size\n",
        "totalTest = test_generator.n//test_generator.batch_size\n",
        "steps_per_epoch = totalTrain // batch_size\n",
        "validation_steps = totalTest // batch_size\n",
        "steps_per_epoch, validation_steps\n",
        "\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "STEP_SIZE_TRAIN, STEP_SIZE_TEST\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpAfiE1rW58A",
        "outputId": "7c159f75-580f-4416-e7ab-81e31dc5e340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(191, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build neural network architecture"
      ],
      "metadata": {
        "id": "vZNh2ngrs4wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk-LkPOCdGOd",
        "outputId": "f45964b8-7e2a-490e-d2ce-10c3ee34e017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = 2 #the number of class\n",
        "headModel = baseModel.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(K, activation=None)(headModel)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n"
      ],
      "metadata": {
        "id": "zr-rUlA7P4P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define activation functions and losses"
      ],
      "metadata": {
        "id": "tSxw5nLwtBap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_evidence(logits):\n",
        "    return tf.nn.relu(logits)\n",
        "\n",
        "def exp_evidence(logits):\n",
        "    return tf.exp(tf.clip_by_value(logits,-10,10))\n",
        "\n",
        "def softplus_evidence(logits):\n",
        "    return tf.nn.softplus(logits)\n",
        "\n",
        "def KL(alpha):\n",
        "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
        "    S_alpha = tf.reduce_sum(input_tensor=alpha,axis=1,keepdims=True)\n",
        "    S_beta = tf.reduce_sum(input_tensor=beta,axis=1,keepdims=True)\n",
        "    lnB = tf.math.lgamma(S_alpha) - tf.reduce_sum(input_tensor=tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
        "    lnB_uni = tf.reduce_sum(input_tensor=tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
        "\n",
        "    dg0 = tf.math.digamma(S_alpha)\n",
        "    dg1 = tf.math.digamma(alpha)\n",
        "\n",
        "    kl = tf.reduce_sum(input_tensor=(alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
        "    return kl\n",
        "\n",
        "def mse_loss(p, alpha, global_step, annealing_step):\n",
        "    S = tf.reduce_sum(input_tensor=alpha, axis=1, keepdims=True)\n",
        "    E = alpha - 1\n",
        "    m = alpha / S\n",
        "\n",
        "    A = tf.reduce_sum(input_tensor=(p-m)**2, axis=1, keepdims=True)\n",
        "    B = tf.reduce_sum(input_tensor=alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
        "    annealing_coef = tf.minimum(1.0, tf.cast(global_step/annealing_step,tf.float32))\n",
        "\n",
        "    alp = E*(1-p) + 1\n",
        "    C =  annealing_coef * KL(alp)\n",
        "\n",
        "    return (A + B) + C\n"
      ],
      "metadata": {
        "id": "PPTu8nyqP_wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ev_succ(y_true, y_pred):\n",
        "    evidence = exp_evidence(y_pred)\n",
        "    pred = tf.argmax(input=y_pred, axis=1)\n",
        "    truth = tf.argmax(input=y_true, axis=1)\n",
        "    match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
        "    total_evidence = tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)\n",
        "    mean_ev_succ = tf.reduce_sum(input_tensor=tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)*match) / tf.reduce_sum(input_tensor=match+1e-20)\n",
        "    ev_succ = tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)*match\n",
        "    return ev_succ\n",
        "\n",
        "def ev_fail(y_true, y_pred):\n",
        "    evidence = exp_evidence(y_pred)\n",
        "    pred = tf.argmax(input=y_pred, axis=1)\n",
        "    truth = tf.argmax(input=y_true, axis=1)\n",
        "    match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
        "    total_evidence = tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)\n",
        "    mean_ev_fail = tf.reduce_sum(input_tensor=tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)*(1-match)) / (tf.reduce_sum(input_tensor=tf.abs(1-match))+1e-20)\n",
        "    ev_fail = tf.reduce_sum(input_tensor=evidence,axis=1, keepdims=True)*(1-match)\n",
        "    return ev_fail\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "    annealing_step = NUM_EPOCHS * steps_per_epoch\n",
        "    evidence = exp_evidence(y_pred)\n",
        "    alpha = evidence + 1\n",
        "    prob = alpha/tf.reduce_sum(input_tensor=alpha, axis=1, keepdims=True)\n",
        "\n",
        "    loss = mse_loss(y_true, alpha, globalstep.global_step, annealing_step)\n",
        "\n",
        "    return tf.reduce_mean(input_tensor=loss)\n"
      ],
      "metadata": {
        "id": "FuC5qtzPQO0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalStep(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, epoch_counter=0.0, global_step=0.0):\n",
        "        super(GlobalStep, self).__init__()\n",
        "        sess = tf.compat.v1.keras.backend.get_session()\n",
        "        epoch_counter = tf.Variable(float(epoch_counter), trainable=False,  name='epoch_counter')\n",
        "        global_step = tf.Variable(float(global_step), trainable=False, name='global_step')\n",
        "        sess.run(epoch_counter.initializer)\n",
        "        sess.run(global_step.initializer)\n",
        "        self.sess = sess\n",
        "        self.epoch_counter = epoch_counter;\n",
        "        self.global_step = global_step\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "       op = self.epoch_counter.assign(self.epoch_counter + 1)\n",
        "       self.sess.run(op)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "       op = self.global_step.assign(self.global_step + 1)\n",
        "       self.sess.run(op)\n",
        "\n",
        "    def set_globalstep(self, val):\n",
        "       op = self.global_step.assign(val)\n",
        "       self.sess.run(op)\n",
        "\n",
        "    def set_epoch(self, val):\n",
        "       op = self.epoch_counter.assign(val)\n",
        "       self.sess.run(op)\n",
        "\n",
        "globalstep = GlobalStep()\n"
      ],
      "metadata": {
        "id": "Ibg7qgrVQTiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.legacy.Adam(learning_rate=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
        "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\", ev_succ, ev_fail], run_eagerly=False)\n"
      ],
      "metadata": {
        "id": "pFr_mqIfQVkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "globalstep.sess.run(globalstep.global_step)\n"
      ],
      "metadata": {
        "id": "dDZawOAYQZhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a model"
      ],
      "metadata": {
        "id": "d9ZPFdkYtQ0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Train_Model:\n",
        "    # You should specify a folder to save your trained model\n",
        "    checkpoint = ModelCheckpoint(model_dir +'/pure', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "    H = model.fit_generator(\n",
        "        generator=train_generator,\n",
        "        steps_per_epoch=STEP_SIZE_TRAIN+1,\n",
        "        epochs=NUM_EPOCHS, callbacks=[globalstep, checkpoint])\n"
      ],
      "metadata": {
        "id": "iUOC5toRQeHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict the trained model"
      ],
      "metadata": {
        "id": "Xqf_fc-ytTSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edl_stats(pred, lbs, truth=None, predIdxs=None):\n",
        "  if truth is None:\n",
        "    truth =  np.argmax(lbs, axis=1)\n",
        "  if predIdxs is None:\n",
        "    predIdxs = np.argmax(pred, axis=1)\n",
        "  match = (truth == predIdxs)\n",
        "\n",
        "  return match\n"
      ],
      "metadata": {
        "id": "pcG7ZYwp3epi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_train = []\n",
        "P_train = []\n",
        "E_train = []\n",
        "M_train = []\n",
        "O_train = []\n",
        "\n",
        "train_generator.reset()\n",
        "for i in tqdm(range(STEP_SIZE_TRAIN+1)):\n",
        "    img, lbs = train_generator.next()\n",
        "    pred_train = model.predict(img)\n",
        "    predIdxs = np.argmax(pred_train, axis=1)\n",
        "    truth_train =  np.argmax(lbs, axis=1)\n",
        "    match_train = edl_stats(pred_train, lbs, truth_train, predIdxs)\n",
        "    M_train.append(match_train)\n",
        "    T_train.append(truth_train)\n",
        "    P_train.append(predIdxs)\n",
        "    O_train.append(pred_train)\n",
        "\n",
        "pred_train = np.concatenate(P_train)\n",
        "output_train = np.concatenate(O_train)\n",
        "match_train = np.concatenate(M_train)\n",
        "truth_train = np.concatenate(T_train)\n",
        "ev_train = np.exp(output_train)\n"
      ],
      "metadata": {
        "id": "JQWaKMA6zD2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the Train uncertainty"
      ],
      "metadata": {
        "id": "G5MPmwNra54f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tot_ev_train = ev_train.sum(axis=1)\n",
        "train_u = 2/(2+tot_ev_train)\n"
      ],
      "metadata": {
        "id": "deWvukoizFz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = []\n",
        "P = []\n",
        "E = []\n",
        "M = []\n",
        "O = []\n",
        "\n",
        "test_generator.reset()\n",
        "for i in tqdm(range(STEP_SIZE_TEST)):\n",
        "    img = test_generator.next()\n",
        "    pred = model.predict(img)\n",
        "    predIdxs = np.argmax(pred, axis=1)\n",
        "\n",
        "    P.append(predIdxs)\n",
        "    O.append(pred)\n",
        "\n",
        "pred = np.concatenate(P)\n",
        "output = np.concatenate(O)\n",
        "truth = np.array(df_test_dataset.apply(lambda row: 1 if row.privacy_value==\"public\" else 0, axis=1))\n",
        "ev = np.exp(output)\n"
      ],
      "metadata": {
        "id": "HzEvHv--Qgc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5f306f-915c-49fd-984a-7fdfb332e02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:29<00:00,  1.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the Test uncertainty"
      ],
      "metadata": {
        "id": "6YqxEN4boqob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tot_ev = ev.sum(axis=1)\n",
        "test_u = 2/(2+tot_ev)\n"
      ],
      "metadata": {
        "id": "vsYUp1ywqVzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}